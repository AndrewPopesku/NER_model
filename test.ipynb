{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOmT/0NpYuVYpYrNb+8FZBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewPopesku/NER_model/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knr7dkDnFx4y",
        "outputId": "a2486306-e5f5-4327-89f6-5121132d6270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=a9679b091233630f4008cb30a8b44555a803091511e15bb841470e655232ede7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt\n",
            "Successfully installed docopt-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRbGuU_E_A7"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "from vocab import *\n",
        "\n",
        "sentences, tags = read_corpus(\"data.txt\")\n",
        "max_dict_size = max([len(sent) for sent in sentences])\n",
        "max_tag_size = max([len(sent) for sent in tags])\n",
        "sent_vocab = Vocab.build(sentences, max_dict_size, 1, is_tags=False)\n",
        "tag_vocab = Vocab.build(tags, max_tag_size, 1, is_tags=True)\n",
        "sent_vocab.save(\"vocab/sent_vocab.json\")\n",
        "tag_vocab.save(\"vocab/tag_vocab.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vocab import Vocab\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bilstm_crf\n",
        "import utils\n",
        "import random\n",
        "\n",
        "args = {\n",
        "    'SENT_VOCAB': 'vocab/sent_vocab.json',\n",
        "    'TAG_VOCAB': 'vocab/tag_vocab.json',\n",
        "    'TRAIN': 'data.txt',\n",
        "    '--batch-size': '32',\n",
        "    '--max-epoch': '10',\n",
        "    '--log-every': '10',\n",
        "    '--validation-every': '250',\n",
        "    '--model-save-path': 'model.pth',\n",
        "    '--optimizer-save-path': 'optimizer.pth',\n",
        "    '--cuda': False,\n",
        "    '--dropout-rate': '0.5',\n",
        "    '--embed-size': '256',\n",
        "    '--hidden-size': '256',\n",
        "    '--lr': '0.001',\n",
        "    '--clip_max_norm': '5.0',\n",
        "    '--patience-threshold': '0.98',\n",
        "    '--max-patience': '4',\n",
        "    '--max-decay': '4',\n",
        "    '--lr-decay': '0.5'\n",
        "}\n",
        "\n",
        "def train(args):\n",
        "    \"\"\" Training BiLSTMCRF model\n",
        "    Args:\n",
        "        args: dict that contains options in command\n",
        "    \"\"\"\n",
        "    sent_vocab = Vocab.load(args['SENT_VOCAB'])\n",
        "    tag_vocab = Vocab.load(args['TAG_VOCAB'])\n",
        "    train_data, dev_data = utils.generate_train_dev_dataset(args['TRAIN'], sent_vocab, tag_vocab)\n",
        "    print('num of training examples: %d' % (len(train_data)))\n",
        "    print('num of development examples: %d' % (len(dev_data)))\n",
        "\n",
        "    max_epoch = int(args['--max-epoch'])\n",
        "    log_every = int(args['--log-every'])\n",
        "    validation_every = int(args['--validation-every'])\n",
        "    model_save_path = args['--model-save-path']\n",
        "    optimizer_save_path = args['--optimizer-save-path']\n",
        "    min_dev_loss = float('inf')\n",
        "    device = torch.device('cuda' if args['--cuda'] else 'cpu')\n",
        "    patience, decay_num = 0, 0\n",
        "\n",
        "    model = bilstm_crf.BiLSTMCRF(sent_vocab, tag_vocab, float(args['--dropout-rate']), int(args['--embed-size']),\n",
        "                                 int(args['--hidden-size'])).to(device)\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, 0, 0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n",
        "    train_iter = 0  # train iter num\n",
        "    record_loss_sum, record_tgt_word_sum, record_batch_size = 0, 0, 0  # sum in one training log\n",
        "    cum_loss_sum, cum_tgt_word_sum, cum_batch_size = 0, 0, 0  # sum in one validation log\n",
        "    record_start, cum_start = time.time(), time.time()\n",
        "\n",
        "    print('start training...')\n",
        "    for epoch in range(max_epoch):\n",
        "        for sentences, tags in utils.batch_iter(train_data, batch_size=int(args['--batch-size'])):\n",
        "            train_iter += 1\n",
        "            current_batch_size = len(sentences)\n",
        "            sentences, sent_lengths = utils.pad(sentences, sent_vocab[sent_vocab.PAD], device)\n",
        "            tags, _ = utils.pad(tags, tag_vocab[tag_vocab.PAD], device)\n",
        "\n",
        "            # back propagation\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = model(sentences, tags, sent_lengths)  # shape: (b,)\n",
        "            loss = batch_loss.mean()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(args['--clip_max_norm']))\n",
        "            optimizer.step()\n",
        "\n",
        "            record_loss_sum += batch_loss.sum().item()\n",
        "            record_batch_size += current_batch_size\n",
        "            record_tgt_word_sum += sum(sent_lengths)\n",
        "\n",
        "            cum_loss_sum += batch_loss.sum().item()\n",
        "            cum_batch_size += current_batch_size\n",
        "            cum_tgt_word_sum += sum(sent_lengths)\n",
        "\n",
        "            # if train_iter % log_every == 0:\n",
        "            print('log: epoch %d, iter %d, %.1f words/sec, avg_loss %f, time %.1f sec' %\n",
        "                  (epoch + 1, train_iter, record_tgt_word_sum / (time.time() - record_start),\n",
        "                    record_loss_sum / record_batch_size, time.time() - record_start))\n",
        "            record_loss_sum, record_batch_size, record_tgt_word_sum = 0, 0, 0\n",
        "            record_start = time.time()\n",
        "\n",
        "            if train_iter % validation_every == 0:\n",
        "                print('dev: epoch %d, iter %d, %.1f words/sec, avg_loss %f, time %.1f sec' %\n",
        "                      (epoch + 1, train_iter, cum_tgt_word_sum / (time.time() - cum_start),\n",
        "                       cum_loss_sum / cum_batch_size, time.time() - cum_start))\n",
        "                cum_loss_sum, cum_batch_size, cum_tgt_word_sum = 0, 0, 0\n",
        "\n",
        "                dev_loss = cal_dev_loss(model, dev_data, 64, sent_vocab, tag_vocab, device)\n",
        "                if dev_loss < min_dev_loss * float(args['--patience-threshold']):\n",
        "                    min_dev_loss = dev_loss\n",
        "                    model.save(model_save_path)\n",
        "                    torch.save(optimizer.state_dict(), optimizer_save_path)\n",
        "                    patience = 0\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience == int(args['--max-patience']):\n",
        "                        decay_num += 1\n",
        "                        if decay_num == int(args['--max-decay']):\n",
        "                            print('Early stop. Save result model to %s' % model_save_path)\n",
        "                            return\n",
        "                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n",
        "                        model = bilstm_crf.BiLSTMCRF.load(model_save_path, device)\n",
        "                        optimizer.load_state_dict(torch.load(optimizer_save_path))\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                        patience = 0\n",
        "                print('dev: epoch %d, iter %d, dev_loss %f, patience %d, decay_num %d' %\n",
        "                      (epoch + 1, train_iter, dev_loss, patience, decay_num))\n",
        "                cum_start = time.time()\n",
        "                if train_iter % log_every == 0:\n",
        "                    record_start = time.time()\n",
        "    print('Reached %d epochs, Save result model to %s' % (max_epoch, model_save_path))\n",
        "\n",
        "train(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDn-QO5oFqke",
        "outputId": "632f197e-9b0f-4063-fa91-19e6e31b31d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of training examples: 71\n",
            "num of development examples: 18\n",
            "start training...\n",
            "log: epoch 1, iter 1, 352.1 words/sec, avg_loss 1346.711548, time 38.5 sec\n",
            "log: epoch 1, iter 2, 532.0 words/sec, avg_loss 1615.376343, time 30.7 sec\n",
            "log: epoch 1, iter 3, 502.9 words/sec, avg_loss 1457.900949, time 6.4 sec\n",
            "log: epoch 2, iter 4, 486.8 words/sec, avg_loss 1537.272095, time 32.1 sec\n",
            "log: epoch 2, iter 5, 348.4 words/sec, avg_loss 1574.856445, time 46.3 sec\n",
            "log: epoch 2, iter 6, 1061.3 words/sec, avg_loss 583.607806, time 1.2 sec\n",
            "log: epoch 3, iter 7, 370.8 words/sec, avg_loss 1595.929077, time 45.4 sec\n",
            "log: epoch 3, iter 8, 485.5 words/sec, avg_loss 1196.933472, time 26.8 sec\n",
            "log: epoch 3, iter 9, 1011.0 words/sec, avg_loss 1252.927734, time 3.2 sec\n",
            "log: epoch 4, iter 10, 470.7 words/sec, avg_loss 970.950989, time 27.2 sec\n",
            "log: epoch 4, iter 11, 381.6 words/sec, avg_loss 1232.657471, time 44.7 sec\n",
            "log: epoch 4, iter 12, 1512.4 words/sec, avg_loss 1014.579660, time 2.1 sec\n",
            "log: epoch 5, iter 13, 470.6 words/sec, avg_loss 787.453979, time 26.1 sec\n",
            "log: epoch 5, iter 14, 376.5 words/sec, avg_loss 928.261353, time 43.0 sec\n",
            "log: epoch 5, iter 15, 796.7 words/sec, avg_loss 1361.835100, time 5.8 sec\n",
            "log: epoch 6, iter 16, 492.0 words/sec, avg_loss 810.494995, time 27.2 sec\n",
            "log: epoch 6, iter 17, 352.7 words/sec, avg_loss 913.369446, time 44.7 sec\n",
            "log: epoch 6, iter 18, 1909.1 words/sec, avg_loss 1035.615374, time 2.1 sec\n",
            "log: epoch 7, iter 19, 531.1 words/sec, avg_loss 954.949036, time 30.4 sec\n",
            "log: epoch 7, iter 20, 448.6 words/sec, avg_loss 725.210632, time 27.2 sec\n",
            "log: epoch 7, iter 21, 520.1 words/sec, avg_loss 1216.483398, time 9.0 sec\n",
            "log: epoch 8, iter 22, 370.5 words/sec, avg_loss 929.203857, time 43.6 sec\n",
            "log: epoch 8, iter 23, 469.8 words/sec, avg_loss 848.573547, time 31.2 sec\n",
            "log: epoch 8, iter 24, 442.3 words/sec, avg_loss 625.991281, time 5.1 sec\n",
            "log: epoch 9, iter 25, 690.9 words/sec, avg_loss 856.416504, time 21.8 sec\n",
            "log: epoch 9, iter 26, 376.6 words/sec, avg_loss 944.669434, time 43.3 sec\n",
            "log: epoch 9, iter 27, 438.3 words/sec, avg_loss 508.988107, time 4.0 sec\n",
            "log: epoch 10, iter 28, 607.3 words/sec, avg_loss 663.485962, time 19.4 sec\n",
            "log: epoch 10, iter 29, 362.0 words/sec, avg_loss 985.704346, time 47.7 sec\n",
            "log: epoch 10, iter 30, 1098.9 words/sec, avg_loss 1071.906250, time 3.7 sec\n",
            "Reached 10 epochs, Save result model to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuNd-fgzGU2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}