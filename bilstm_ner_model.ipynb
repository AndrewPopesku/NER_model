{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c488a2",
   "metadata": {},
   "source": [
    "# BiLSTM Named Entity Recognition (NER) Model for CoNLL-2003 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cf922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 16:44:12.695239: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 16:44:12.699727: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 16:44:12.763482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 16:44:14.034446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06771d",
   "metadata": {},
   "source": [
    "## Load and Preprocess the CoNLL-2003 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8252b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the CoNLL-2003 dataset\n",
    "def load_data(filepath):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('-DOCSTART-') or line == '\\n':\n",
    "                if sentence:\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            splits = line.split()\n",
    "            sentence.append((splits[0], splits[1], splits[3]))  # Append word, POS, and NER tag\n",
    "    if sentence:\n",
    "        data.append(sentence)\n",
    "    return data\n",
    "\n",
    "train_data = load_data('dataset/train.txt')\n",
    "val_data = load_data('dataset/valid.txt')\n",
    "test_data = load_data('dataset/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41a0a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>representative</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>European</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Union</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>veterinary</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>committee</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Werner</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Zwingmann</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>consumers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>should</td>\n",
       "      <td>MD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>buy</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sheepmeat</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>countries</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>other</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>than</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Britain</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>until</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>scientific</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>advice</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>clearer</td>\n",
       "      <td>JJR</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0    1      2\n",
       "0          Germany  NNP  B-LOC\n",
       "1               's  POS      O\n",
       "2   representative   NN      O\n",
       "3               to   TO      O\n",
       "4              the   DT      O\n",
       "5         European  NNP  B-ORG\n",
       "6            Union  NNP  I-ORG\n",
       "7               's  POS      O\n",
       "8       veterinary   JJ      O\n",
       "9        committee   NN      O\n",
       "10          Werner  NNP  B-PER\n",
       "11       Zwingmann  NNP  I-PER\n",
       "12            said  VBD      O\n",
       "13              on   IN      O\n",
       "14       Wednesday  NNP      O\n",
       "15       consumers  NNS      O\n",
       "16          should   MD      O\n",
       "17             buy   VB      O\n",
       "18       sheepmeat   NN      O\n",
       "19            from   IN      O\n",
       "20       countries  NNS      O\n",
       "21           other   JJ      O\n",
       "22            than   IN      O\n",
       "23         Britain  NNP  B-LOC\n",
       "24           until   IN      O\n",
       "25             the   DT      O\n",
       "26      scientific   JJ      O\n",
       "27          advice   NN      O\n",
       "28             was  VBD      O\n",
       "29         clearer  JJR      O\n",
       "30               .    .      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92335e",
   "metadata": {},
   "source": [
    "## Extract Words and Tags from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56c69021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract words and tags from the dataset\n",
    "def extract_words_and_tags(data):\n",
    "    words = list(set([word for sentence in data for word, _, _ in sentence]))\n",
    "    tags = list(set([tag for sentence in data for _, _, tag in sentence]))\n",
    "    return words, tags\n",
    "\n",
    "words, tags = extract_words_and_tags(train_data + val_data + test_data)\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx['UNK'] = 1  # Unknown words\n",
    "word2idx['PAD'] = 0  # Padding\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "max_len = 75  # Maximum sequence length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a59b6",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54481bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(data, word2idx: dict, tag2idx: dict, max_len: int):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence_words = []\n",
    "        sentence_tags = []\n",
    "        for word, _, tag in sentence:\n",
    "            sentence_words.append(word2idx.get(word, 1))\n",
    "            sentence_tags.append(tag2idx[tag])\n",
    "        X.append(sentence_words)\n",
    "        y.append(sentence_tags)\n",
    "    \n",
    "    X = pad_sequences(X, maxlen=max_len, padding='post')\n",
    "    y = pad_sequences(y, maxlen=max_len, padding='post')\n",
    "    y = [to_categorical(i, num_classes=len(tag2idx)) for i in y]\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = preprocess_data(train_data, word2idx, tag2idx, max_len)\n",
    "X_val, y_val = preprocess_data(val_data, word2idx, tag2idx, max_len)\n",
    "X_test, y_test = preprocess_data(test_data, word2idx, tag2idx, max_len)\n",
    "y_val[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ecff",
   "metadata": {},
   "source": [
    "## Define the BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa889da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2idx), output_dim=50, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(TimeDistributed(Dense(len(tag2idx), activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4fe36",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3f68ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 119ms/step - accuracy: 0.9302 - loss: 0.2861 - val_accuracy: 0.9740 - val_loss: 0.0836\n",
      "Epoch 2/5\n",
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 117ms/step - accuracy: 0.9825 - loss: 0.0567 - val_accuracy: 0.9865 - val_loss: 0.0479\n",
      "Epoch 3/5\n",
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 155ms/step - accuracy: 0.9942 - loss: 0.0223 - val_accuracy: 0.9901 - val_loss: 0.0365\n",
      "Epoch 4/5\n",
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 165ms/step - accuracy: 0.9974 - loss: 0.0107 - val_accuracy: 0.9904 - val_loss: 0.0349\n",
      "Epoch 5/5\n",
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 164ms/step - accuracy: 0.9985 - loss: 0.0061 - val_accuracy: 0.9913 - val_loss: 0.0326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22fef5",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5401961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.9877 - loss: 0.0442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04246332868933678, 0.9883927702903748]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb834440",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dab9fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"bilstm_ner_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e63ddceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word2idx dictionary\n",
    "with open('word2idx.json', 'w') as json_file:\n",
    "    json.dump(word2idx, json_file, indent=4)\n",
    "\n",
    "# Create idx2tag dictionary\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "# Save the idx2tag dictionary with integer keys\n",
    "with open('idx2tag.json', 'w') as json_file:\n",
    "    json.dump({int(k): v for k, v in idx2tag.items()}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e8456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
